{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Attention Mechanism Explained\n",
    "\n",
    "This notebook walks through a simple attention mechanism implementation with detailed explanations at each step. We'll use very descriptive variable names and avoid complex math notation to make it accessible for everyone.\n",
    "\n",
    "## What is Attention?\n",
    "\n",
    "At its core, **attention** is about helping a model focus on the most relevant parts of the input when producing each part of the output.\n",
    "\n",
    "Think of it like this: When you're reading a long document and trying to answer a specific question, you don't give equal focus to every word. You *pay attention* to the parts most relevant to your question. Attention mechanisms help neural networks do the same thing.\n",
    "\n",
    "## Scaled Dot-Product Attention\n",
    "\n",
    "We'll implement the simplest form of attention used in the \"Attention Is All You Need\" paper (2017) by Vaswani et al., which introduced the Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Three Key Players: Queries, Keys, and Values\n",
    "\n",
    "Before we dive into code, let's understand the three main components of attention:\n",
    "\n",
    "1. **Queries**: Think of these as \"questions\" or \"search terms\" that help you look for relevant information\n",
    "2. **Keys**: These are like \"labels\" or \"tags\" for different pieces of information\n",
    "3. **Values**: These are the actual information or content you want to extract\n",
    "\n",
    "With attention, we use the similarity between queries and keys to determine how much of each value to extract.\n",
    "\n",
    "### Real-World Analogy: Library Search\n",
    "\n",
    "Imagine you're at a library:\n",
    "- **Query**: The topic you're researching (e.g., \"French cuisine\")\n",
    "- **Keys**: The titles or categories of books on the shelves\n",
    "- **Values**: The actual content inside each book\n",
    "\n",
    "You compare your query (\"French cuisine\") to all the keys (book titles), find the most relevant ones, and then extract information from their values (book contents). Books with titles more similar to your query get more of your attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def simple_attention_mechanism(query_vectors, key_vectors, value_vectors):\n",
    "    \"\"\"\n",
    "    A simple implementation of scaled dot-product attention.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    query_vectors : numpy array of shape (batch_size, query_sequence_length, feature_dimension)\n",
    "        The queries - what we're using to search for relevant information\n",
    "        \n",
    "    key_vectors : numpy array of shape (batch_size, key_sequence_length, feature_dimension)\n",
    "        The keys - what we're searching through to find matches to our queries\n",
    "        \n",
    "    value_vectors : numpy array of shape (batch_size, key_sequence_length, value_dimension)\n",
    "        The values - the actual information we want to extract based on query-key matching\n",
    "        Note: key_sequence_length must match value_sequence_length as they're paired\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weighted_sum_of_values : numpy array of shape (batch_size, query_sequence_length, value_dimension)\n",
    "        Each query gets its own weighted sum of values\n",
    "        \n",
    "    attention_weights : numpy array of shape (batch_size, query_sequence_length, key_sequence_length)\n",
    "        How much each query attended to each key (useful for visualization)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Calculate similarity between queries and keys using dot product\n",
    "    # For each query, we compute how similar it is to each key\n",
    "    # Higher dot product = more similarity = more attention\n",
    "    similarity_scores = np.matmul(query_vectors, np.transpose(key_vectors, (0, 2, 1)))\n",
    "    \n",
    "    # Step 2: Scale the similarity scores to prevent extremely small gradients\n",
    "    # We divide by square root of the dimensionality of the key vectors\n",
    "    # This is important for stable training, especially with larger dimensions\n",
    "    dimension_of_key_vectors = key_vectors.shape[-1]\n",
    "    scaled_similarity_scores = similarity_scores / np.sqrt(dimension_of_key_vectors)\n",
    "    \n",
    "    # Step 3: Convert similarity scores to probabilities using softmax\n",
    "    # This ensures all attention weights for each query sum to 1\n",
    "    # Each query now has a probability distribution over all keys\n",
    "    \n",
    "    # First, we need a softmax function\n",
    "    def softmax(x):\n",
    "        # Subtract max for numerical stability (prevents overflow)\n",
    "        shifted_x = x - np.max(x, axis=-1, keepdims=True)\n",
    "        # Calculate exponential values\n",
    "        exp_x = np.exp(shifted_x)\n",
    "        # Normalize to get probabilities\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = softmax(scaled_similarity_scores)\n",
    "    \n",
    "    # Step 4: Use attention weights to create a weighted sum of values\n",
    "    # This gives us a new representation focused on the most relevant parts\n",
    "    weighted_sum_of_values = np.matmul(attention_weights, value_vectors)\n",
    "    \n",
    "    return weighted_sum_of_values, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding with a Simple Example\n",
    "\n",
    "Let's create a tiny example to visualize exactly what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a simple example with small vectors\n",
    "batch_size = 1                  # Just one example for simplicity\n",
    "query_sequence_length = 2       # We have 2 queries (imagine 2 words in an output sentence)\n",
    "key_value_sequence_length = 3   # We have 3 keys/values (imagine 3 words in an input sentence)\n",
    "feature_dimension = 4           # Each vector has 4 features\n",
    "value_dimension = 6             # Values have 6 features (could be different from query/key)\n",
    "\n",
    "# Create sample queries, keys, and values\n",
    "# In real applications, these would come from learned embeddings or transformations\n",
    "example_queries = np.random.randn(batch_size, query_sequence_length, feature_dimension)\n",
    "example_keys = np.random.randn(batch_size, key_value_sequence_length, feature_dimension)\n",
    "example_values = np.random.randn(batch_size, key_value_sequence_length, value_dimension)\n",
    "\n",
    "# Run our attention mechanism\n",
    "attention_output, attention_weights = simple_attention_mechanism(\n",
    "    example_queries, example_keys, example_values)\n",
    "\n",
    "print(f\"Shape of attention output: {attention_output.shape}\")\n",
    "print(f\"Shape of attention weights: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Attention Weights\n",
    "\n",
    "Let's visualize the attention weights to understand what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the attention weights\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(attention_weights[0], cmap='viridis')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Keys (Input Words)')\n",
    "plt.ylabel('Queries (Output Words)')\n",
    "plt.title('Attention Weights: How Much Each Query Focuses on Each Key')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(query_sequence_length):\n",
    "    for j in range(key_value_sequence_length):\n",
    "        plt.text(j, i, f'{attention_weights[0, i, j]:.2f}',\n",
    "                 ha=\"center\", va=\"center\", color=\"white\" if attention_weights[0, i, j] < 0.5 else \"black\")\n",
    "\n",
    "# Use more intuitive labels\n",
    "plt.xticks(range(key_value_sequence_length), [f'Input {i+1}' for i in range(key_value_sequence_length)])\n",
    "plt.yticks(range(query_sequence_length), [f'Output {i+1}' for i in range(query_sequence_length)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Realistic Example: Sentence Translation\n",
    "\n",
    "Let's create a more realistic example where we use attention for translating a simple sentence.\n",
    "\n",
    "We'll simulate a very simple English → Spanish translation task.\n",
    "\n",
    "In this example, we'll manually create our vectors rather than training a model, but the attention mechanism works the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a simple English sentence and its Spanish translation\n",
    "english_sentence = [\"The\", \"cat\", \"is\", \"sleeping\", \"on\", \"the\", \"mat\"]\n",
    "spanish_sentence = [\"El\", \"gato\", \"está\", \"durmiendo\", \"en\", \"la\", \"estera\"]\n",
    "\n",
    "# For simplicity, we'll manually create word embeddings\n",
    "# In a real system, these would be learned by the model\n",
    "# We'll use 5-dimensional embeddings\n",
    "embedding_dimension = 5\n",
    "\n",
    "# Create random but fixed embeddings for our words\n",
    "np.random.seed(123)  # For reproducibility\n",
    "english_embeddings = {word: np.random.randn(embedding_dimension) for word in english_sentence}\n",
    "spanish_embeddings = {word: np.random.randn(embedding_dimension) for word in spanish_sentence}\n",
    "\n",
    "# Convert sentences to sequences of embeddings\n",
    "english_vectors = np.array([english_embeddings[word] for word in english_sentence])\n",
    "spanish_vectors = np.array([spanish_embeddings[word] for word in spanish_sentence])\n",
    "\n",
    "# Reshape for our attention function (add batch dimension)\n",
    "english_vectors = english_vectors.reshape(1, len(english_sentence), embedding_dimension)\n",
    "spanish_vectors = spanish_vectors.reshape(1, len(spanish_sentence), embedding_dimension)\n",
    "\n",
    "# In this example:\n",
    "# - Keys and Values are from the English sentence (source language)\n",
    "# - Queries are from the Spanish sentence (target language)\n",
    "# This simulates attention during translation\n",
    "\n",
    "translation_attention_output, translation_attention_weights = simple_attention_mechanism(\n",
    "    query_vectors=spanish_vectors,   # Target language (what we're generating)\n",
    "    key_vectors=english_vectors,     # Source language (what we're translating from)\n",
    "    value_vectors=english_vectors    # Using same vectors as values for simplicity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the translation attention weights\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(translation_attention_weights[0], cmap='YlOrRd')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('English Words (Source)')\n",
    "plt.ylabel('Spanish Words (Target)')\n",
    "plt.title('Translation Attention Weights: Word Alignment Visualization')\n",
    "\n",
    "# Add text annotations for the weights\n",
    "for i in range(len(spanish_sentence)):\n",
    "    for j in range(len(english_sentence)):\n",
    "        plt.text(j, i, f'{translation_attention_weights[0, i, j]:.2f}',\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"black\" if translation_attention_weights[0, i, j] < 0.15 else \"white\")\n",
    "\n",
    "# Use actual words as labels\n",
    "plt.xticks(range(len(english_sentence)), english_sentence, rotation=45)\n",
    "plt.yticks(range(len(spanish_sentence)), spanish_sentence)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Attention Map\n",
    "\n",
    "In the visualization above, each cell shows how much a Spanish word (target) attends to each English word (source). Brighter/darker colors indicate stronger attention.\n",
    "\n",
    "In an ideal translation system, we would see:\n",
    "- \"El\" attending most to \"The\"\n",
    "- \"gato\" attending most to \"cat\"\n",
    "- \"está durmiendo\" attending most to \"is sleeping\"\n",
    "- And so on...\n",
    "\n",
    "Our random embeddings won't show perfect alignment, but in a trained model, you would see these patterns emerge!\n",
    "\n",
    "## Implementing Attention in TensorFlow/Keras\n",
    "\n",
    "Now let's implement the same attention mechanism using TensorFlow/Keras for use in neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class ScaledDotProductAttention(layers.Layer):\n",
    "    \"\"\"Custom layer implementing scaled dot-product attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, query_tensor, key_tensor, value_tensor, mask=None):\n",
    "        \"\"\"Forward pass for the attention layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        query_tensor: Tensor of shape (..., query_sequence_length, query_dimension)\n",
    "            The queries we use to search through the keys\n",
    "            \n",
    "        key_tensor: Tensor of shape (..., key_sequence_length, key_dimension)\n",
    "            The keys we're searching through (must have same dimension as queries)\n",
    "            \n",
    "        value_tensor: Tensor of shape (..., key_sequence_length, value_dimension)\n",
    "            The values we extract based on attention weights\n",
    "            \n",
    "        mask: Optional tensor for masking certain positions (e.g., padding)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        attention_output: Tensor of shape (..., query_sequence_length, value_dimension)\n",
    "            The weighted combination of values\n",
    "            \n",
    "        attention_weights: Tensor of shape (..., query_sequence_length, key_sequence_length)\n",
    "            The attention weights for visualization\n",
    "        \"\"\"\n",
    "        # Step 1: Calculate similarity scores using tensor dot product\n",
    "        # We transpose the last two dimensions of key_tensor to align with query_tensor\n",
    "        query_key_similarity_scores = tf.matmul(query_tensor, key_tensor, transpose_b=True)\n",
    "        \n",
    "        # Step 2: Scale the similarity scores by square root of dimension\n",
    "        # This prevents the softmax from having extremely small gradients\n",
    "        dimension_of_keys = tf.cast(tf.shape(key_tensor)[-1], tf.float32)\n",
    "        scaled_similarity_scores = query_key_similarity_scores / tf.math.sqrt(dimension_of_keys)\n",
    "        \n",
    "        # Step 3: Apply mask if provided (useful for padding or future-masking)\n",
    "        if mask is not None:\n",
    "            # Add a very large negative value to masked positions\n",
    "            # This forces softmax to give ~0 attention to those positions\n",
    "            scaled_similarity_scores += (mask * -1e9)  \n",
    "        \n",
    "        # Step 4: Convert to probabilities with softmax\n",
    "        # This makes all attention weights for each query sum to 1\n",
    "        attention_probability_weights = tf.nn.softmax(scaled_similarity_scores, axis=-1)\n",
    "        \n",
    "        # Step 5: Create weighted sum of values using attention weights\n",
    "        # Each query gets its own custom weighted blend of values\n",
    "        attention_weighted_output = tf.matmul(attention_probability_weights, value_tensor)\n",
    "        \n",
    "        return attention_weighted_output, attention_probability_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our TensorFlow attention layer with the same translation example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert our numpy arrays to TensorFlow tensors\n",
    "tf_spanish_vectors = tf.constant(spanish_vectors, dtype=tf.float32)\n",
    "tf_english_vectors = tf.constant(english_vectors, dtype=tf.float32)\n",
    "\n",
    "# Create our attention layer\n",
    "attention_layer = ScaledDotProductAttention()\n",
    "\n",
    "# Apply the attention\n",
    "tf_output, tf_weights = attention_layer(\n",
    "    query_tensor=tf_spanish_vectors,\n",
    "    key_tensor=tf_english_vectors,\n",
    "    value_tensor=tf_english_vectors\n",
    ")\n",
    "\n",
    "# Convert back to numpy for visualization\n",
    "tf_attention_weights = tf_weights.numpy()\n",
    "\n",
    "# Compare with our previous implementation\n",
    "print(\"Maximum absolute difference between implementations:\", \n",
    "      np.max(np.abs(tf_attention_weights - translation_attention_weights)))\n",
    "\n",
    "# They should be very close (small differences due to floating point precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Multi-Head Attention Mechanism\n",
    "\n",
    "Now that we understand the basic attention mechanism, let's implement Multi-Head Attention, which is a key component of Transformers.\n",
    "\n",
    "**Why Multiple Attention Heads?**\n",
    "\n",
    "Using multiple attention heads allows the model to focus on different aspects of the input simultaneously. For example:\n",
    "- One head might focus on grammatical relationships\n",
    "- Another might focus on semantic meanings\n",
    "- A third might focus on contextual clues\n",
    "\n",
    "This is like having multiple people read the same document, each focusing on different aspects, and then combining their insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \"\"\"Multi-head attention as described in 'Attention Is All You Need'.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dimension, number_of_attention_heads):\n",
    "        \"\"\"Initialize the multi-head attention layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        embedding_dimension: Integer\n",
    "            Dimension of the input embeddings\n",
    "            \n",
    "        number_of_attention_heads: Integer\n",
    "            Number of parallel attention heads to use\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.number_of_attention_heads = number_of_attention_heads\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        \n",
    "        # Ensure the embedding dimension is divisible by the number of heads\n",
    "        assert embedding_dimension % number_of_attention_heads == 0, \\\n",
    "            f\"Embedding dimension {embedding_dimension} must be divisible by number of heads {number_of_attention_heads}\"\n",
    "        \n",
    "        # Calculate dimension per head (how we'll split the embedding)\n",
    "        self.dimension_per_attention_head = embedding_dimension // number_of_attention_heads\n",
    "        \n",
    "        # Create linear transformations for queries, keys, and values\n",
    "        # These learn different projections for each attention head\n",
    "        self.query_projection_layer = layers.Dense(embedding_dimension)\n",
    "        self.key_projection_layer = layers.Dense(embedding_dimension)\n",
    "        self.value_projection_layer = layers.Dense(embedding_dimension)\n",
    "        \n",
    "        # Final output projection combines outputs from all heads\n",
    "        self.output_projection_layer = layers.Dense(embedding_dimension)\n",
    "        \n",
    "        # The basic attention mechanism we'll use for each head\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention()\n",
    "    \n",
    "    def split_heads(self, input_tensor, batch_size):\n",
    "        \"\"\"Split the embedding dimension into multiple heads.\n",
    "        \n",
    "        This reshapes the input tensor to separate out the head dimension\n",
    "        and then transposes to get the right shape for attention calculation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_tensor: Tensor of shape (batch_size, sequence_length, embedding_dimension)\n",
    "            The input embeddings to split into heads\n",
    "            \n",
    "        batch_size: Integer\n",
    "            Batch size of the input\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        split_tensor: Tensor of shape (batch_size, num_heads, sequence_length, dim_per_head)\n",
    "            The input tensor reshaped to separate out the attention heads\n",
    "        \"\"\"\n",
    "        # Reshape to separate the embedding dimension into heads\n",
    "        # From: (batch_size, sequence_length, embedding_dimension)\n",
    "        # To: (batch_size, sequence_length, number_of_attention_heads, dimension_per_attention_head)\n",
    "        split_tensor = tf.reshape(\n",
    "            input_tensor,\n",
    "            (batch_size, -1, self.number_of_attention_heads, self.dimension_per_attention_head)\n",
    "        )\n",
    "        \n",
    "        # Transpose to get shape (batch_size, number_of_attention_heads, sequence_length, dimension_per_attention_head)\n",
    "        # This puts the head dimension where we need it for parallel attention calculation\n",
    "        return tf.transpose(split_tensor, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, query_input, key_input, value_input, mask=None):\n",
    "        \"\"\"Forward pass for multi-head attention.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        query_input: Tensor of shape (batch_size, query_sequence_length, embedding_dimension)\n",
    "            Input tensor for queries\n",
    "            \n",
    "        key_input: Tensor of shape (batch_size, key_sequence_length, embedding_dimension)\n",
    "            Input tensor for keys\n",
    "            \n",
    "        value_input: Tensor of shape (batch_size, value_sequence_length, embedding_dimension)\n",
    "            Input tensor for values\n",
    "            \n",
    "        mask: Optional tensor for masking certain positions\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output: Tensor of shape (batch_size, query_sequence_length, embedding_dimension)\n",
    "            The multi-head attention output\n",
    "            \n",
    "        attention_weights: Dictionary of attention weights from each head\n",
    "            Useful for visualization and analysis\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(query_input)[0]\n",
    "        \n",
    "        # Step 1: Apply linear projections to create queries, keys, and values\n",
    "        # These projections create different representations for each head\n",
    "        query_projections = self.query_projection_layer(query_input)\n",
    "        key_projections = self.key_projection_layer(key_input)\n",
    "        value_projections = self.value_projection_layer(value_input)\n",
    "        \n",
    "        # Step 2: Split projections into multiple heads\n",
    "        # This allows each head to focus on different aspects of the input\n",
    "        query_multi_head = self.split_heads(query_projections, batch_size)\n",
    "        key_multi_head = self.split_heads(key_projections, batch_size)\n",
    "        value_multi_head = self.split_heads(value_projections, batch_size)\n",
    "        \n",
    "        # Step 3: Apply scaled dot-product attention for each head\n",
    "        # Each head calculates its own attention weights and weighted outputs\n",
    "        attention_output_per_head, attention_weights_per_head = self.scaled_dot_product_attention(\n",
    "            query_multi_head, key_multi_head, value_multi_head, mask)\n",
    "        \n",
    "        # Step 4: Transpose and reshape to combine all heads' outputs\n",
    "        # First transpose from: (batch_size, num_heads, seq_len, dim_per_head)\n",
    "        # To: (batch_size, seq_len, num_heads, dim_per_head)\n",
    "        transposed_attention_output = tf.transpose(attention_output_per_head, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # Reshape to: (batch_size, seq_len, embedding_dimension)\n",
    "        # This combines all the heads' outputs back into the original embedding dimension\n",
    "        combined_attention_output = tf.reshape(\n",
    "            transposed_attention_output,\n",
    "            (batch_size, -1, self.embedding_dimension)\n",
    "        )\n",
    "        \n",
    "        # Step 5: Apply final output projection\n",
    "        # This learns how to best combine the outputs from all heads\n",
    "        final_output = self.output_projection_layer(combined_attention_output)\n",
    "        \n",
    "        # Create a dictionary of attention weights from each head for visualization\n",
    "        attention_weights_dict = {f\"head_{i+1}\": attention_weights_per_head[:, i, :, :]\n",
    "                                 for i in range(self.number_of_attention_heads)}\n",
    "        \n",
    "        return final_output, attention_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Let's test the multi-head attention with our translation example\n",
    "embedding_dimension = 5  # Matches our example vectors\n",
    "number_of_heads = 1      # For simplicity first, will try more later\n",
    "\n",
    "# Create multi-head attention layer\n",
    "multi_head_attention = MultiHeadAttention(\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    number_of_attention_heads=number_of_heads\n",
    ")\n",
    "\n",
    "# Apply multi-head attention to our translation example\n",
    "mha_output, mha_weights_dict = multi_head_attention(\n",
    "    query_input=tf_spanish_vectors,\n",
    "    key_input=tf_english_vectors,\n",
    "    value_input=tf_english_vectors\n",
    ")\n",
    "\n",
    "print(f\"Multi-head attention output shape: {mha_output.shape}\")\n",
    "print(f\"Attention weights for head 1 shape: {mha_weights_dict['head_1'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the attention weights from the first head\n",
    "head1_weights = mha_weights_dict['head_1'].numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(head1_weights[0], cmap='plasma')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('English Words (Source)')\n",
    "plt.ylabel('Spanish Words (Target)')\n",
    "plt.title('Multi-Head Attention (Head 1): Word Alignment Visualization')\n",
    "\n",
    "# Add text annotations for the weights\n",
    "for i in range(len(spanish_sentence)):\n",
    "    for j in range(len(english_sentence)):\n",
    "        plt.text(j, i, f'{head1_weights[0, i, j]:.2f}',\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"black\" if head1_weights[0, i, j] < 0.15 else \"white\")\n",
    "\n",
    "# Use actual words as labels\n",
    "plt.xticks(range(len(english_sentence)), english_sentence, rotation=45)\n",
    "plt.yticks(range(len(spanish_sentence)), spanish_sentence)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with Multiple Attention Heads\n",
    "\n",
    "Now let's see what happens when we use multiple attention heads. This better reflects how attention works in actual Transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# We need to adjust embedding dimension to be divisible by number of heads\n",
    "# Let's use a larger embedding size\n",
    "multi_head_embedding_dimension = 10  # Divisible by 2 and 5\n",
    "number_of_heads = 2\n",
    "\n",
    "# Create new random embeddings with the larger dimension\n",
    "np.random.seed(456)\n",
    "english_multi_head_embeddings = {word: np.random.randn(multi_head_embedding_dimension) \n",
    "                                for word in english_sentence}\n",
    "spanish_multi_head_embeddings = {word: np.random.randn(multi_head_embedding_dimension) \n",
    "                                for word in spanish_sentence}\n",
    "\n",
    "# Convert to sequences\n",
    "english_vectors_multi = np.array([english_multi_head_embeddings[word] for word in english_sentence])\n",
    "spanish_vectors_multi = np.array([spanish_multi_head_embeddings[word] for word in spanish_sentence])\n",
    "\n",
    "# Reshape for multi-head attention\n",
    "english_vectors_multi = english_vectors_multi.reshape(1, len(english_sentence), multi_head_embedding_dimension)\n",
    "spanish_vectors_multi = spanish_vectors_multi.reshape(1, len(spanish_sentence), multi_head_embedding_dimension)\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "tf_english_vectors_multi = tf.constant(english_vectors_multi, dtype=tf.float32)\n",
    "tf_spanish_vectors_multi = tf.constant(spanish_vectors_multi, dtype=tf.float32)\n",
    "\n",
    "# Create multi-head attention with 2 heads\n",
    "multi_head_attention = MultiHeadAttention(\n",
    "    embedding_dimension=multi_head_embedding_dimension,\n",
    "    number_of_attention_heads=number_of_heads\n",
    ")\n",
    "\n",
    "# Apply multi-head attention\n",
    "mha_output_multi, mha_weights_multi = multi_head_attention(\n",
    "    query_input=tf_spanish_vectors_multi,\n",
    "    key_input=tf_english_vectors_multi,\n",
    "    value_input=tf_english_vectors_multi\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize attention weights for each head side by side\n",
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "# Plot the first head\n",
    "plt.subplot(1, 2, 1)\n",
    "head1_weights_multi = mha_weights_multi['head_1'].numpy()\n",
    "plt.imshow(head1_weights_multi[0], cmap='YlOrRd')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('English Words (Source)')\n",
    "plt.ylabel('Spanish Words (Target)')\n",
    "plt.title('Head 1 Attention Weights')\n",
    "plt.xticks(range(len(english_sentence)), english_sentence, rotation=45)\n",
    "plt.yticks(range(len(spanish_sentence)), spanish_sentence)\n",
    "\n",
    "# Plot the second head\n",
    "plt.subplot(1, 2, 2)\n",
    "head2_weights_multi = mha_weights_multi['head_2'].numpy()\n",
    "plt.imshow(head2_weights_multi[0], cmap='YlOrRd')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('English Words (Source)')\n",
    "plt.ylabel('Spanish Words (Target)')\n",
    "plt.title('Head 2 Attention Weights')\n",
    "plt.xticks(range(len(english_sentence)), english_sentence, rotation=45)\n",
    "plt.yticks(range(len(spanish_sentence)), spanish_sentence)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Understanding Attention\n",
    "\n",
    "In this notebook, we've broken down the attention mechanism into its simplest form:\n",
    "\n",
    "1. **Basic Attention**: A weighted sum where the weights come from comparing queries to keys\n",
    "   - Calculate similarity scores between queries and keys (dot product)\n",
    "   - Scale scores and convert to probabilities (softmax)\n",
    "   - Use these probabilities to create a weighted sum of values\n",
    "\n",
    "2. **Multi-Head Attention**: Multiple parallel attention operations that let the model focus on different aspects\n",
    "   - Project queries, keys, and values into different subspaces for each head\n",
    "   - Apply basic attention in each subspace\n",
    "   - Combine the results from all heads\n",
    "\n",
    "These attention mechanisms form the foundation of modern Transformer models like those used in BERT, GPT, and other large language models.\n",
    "\n",
    "The key insight is that attention allows the model to dynamically focus on the most relevant parts of the input for each part of the output, rather than treating all input elements equally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
